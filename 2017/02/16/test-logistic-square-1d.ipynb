{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %matplotlib notebook\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %reload_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "m1 = 1.2\n",
    "m0 = 1\n",
    "scale = 1\n",
    "\n",
    "n = 100\n",
    "\n",
    "Y = np.random.binomial(1, .5, n)\n",
    "\n",
    "X = Y * np.random.normal(m1, scale, n) + (1 - Y) * np.random.normal(m0, scale, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe4120c9450>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.distplot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classical Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.686247\n",
      "         Iterations 4\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "lr1_sm = sm.Logit(Y, sm.tools.add_constant(X, prepend=False))\n",
    "lr1_sm_result = lr1_sm.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  100\n",
      "Model:                          Logit   Df Residuals:                       98\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Tue, 14 Feb 2017   Pseudo R-squ.:                0.009955\n",
      "Time:                        17:29:41   Log-Likelihood:                -68.625\n",
      "converged:                       True   LL-Null:                       -69.315\n",
      "                                        LLR p-value:                    0.2401\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.2289      0.197      1.163      0.245      -0.157       0.614\n",
      "const         -0.2318      0.283     -0.819      0.413      -0.787       0.323\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print lr1_sm_result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03870759 -0.03915428]\n",
      " [-0.03915428  0.08016219]]\n"
     ]
    }
   ],
   "source": [
    "print lr1_sm_result.cov_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NBS = 3000\n",
    "\n",
    "wbs_all = []\n",
    "\n",
    "for _ in xrange(NBS):\n",
    "    ind_bs = np.random.choice(range(n), size=n, replace=True)\n",
    "    lr_bs_sm = sm.Logit(Y[ind_bs], sm.tools.add_constant(X[ind_bs], prepend=False))\n",
    "    lr_bs_sm_result = lr_bs_sm.fit(disp=False)\n",
    "    wbs_all.append(lr_bs_sm_result.params.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04728843 -0.04724364]\n",
      " [-0.04724364  0.09065815]]\n"
     ]
    }
   ],
   "source": [
    "wbs_all = np.array(wbs_all)\n",
    "\n",
    "print np.cov(wbs_all.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.JointGrid at 0x7fe4120abb10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.jointplot(wbs_all[:,0], wbs_all[:,1], kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(wbs_all[:,0], wbs_all[:,1], '.')\n",
    "plt.axes().set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Stochastic Inverse Hessian Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import math\n",
    "\n",
    "def logistic_1d_hessian_sgd(X, Y, learn_rate, batch_size, \n",
    "                            burn_in, sample_intv, n_sample, \n",
    "                            hessian_batch, hessian_intv):\n",
    "    \"\"\"\n",
    "    hessian_batch - number of samples used to compute hessain\n",
    "    hessian_intv - number of steps waited till hessian is updated\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros((2, 1))\n",
    "    X = sm.tools.add_constant(X, prepend=False)\n",
    "    \n",
    "    n = len(Y)\n",
    "    Y = 2 * Y - 1\n",
    "    \n",
    "    w_all = []\n",
    "    \n",
    "    for isgd in xrange(burn_in + sample_intv * n_sample + 4):\n",
    "        if isgd % hessian_intv == 0:\n",
    "            H = np.zeros((2, 2))\n",
    "            indH = np.random.choice(range(n), size=hessian_batch, replace=False)\n",
    "            for iH in indH:\n",
    "                Xi = X[iH, :].reshape((-1, 1))\n",
    "                tmp1_dp = float(w.T.dot(Xi)[0])\n",
    "                tmp2 = np.exp(tmp1_dp - 2 * np.logaddexp(0, tmp1_dp))\n",
    "                H = H + tmp2 * (Xi.dot(Xi.T))\n",
    "            H = H / hessian_batch\n",
    "            Hinv = np.linalg.inv(H)\n",
    "        \n",
    "        ind_batch = np.random.choice(range(n), size=batch_size, replace=True)\n",
    "        \n",
    "        Xb = X[ind_batch, :]\n",
    "        Yb = Y[ind_batch]\n",
    "        \n",
    "        tmp1_dp = Xb.dot(w).ravel() * Yb\n",
    "        \n",
    "        tmp2 = np.exp(-np.logaddexp(np.zeros((batch_size,)), tmp1_dp))\n",
    "        \n",
    "        sgd = np.zeros((2, 1))\n",
    "        for ib in xrange(batch_size):\n",
    "            sgd = sgd - Xb[ib, :].reshape((-1, 1)) * tmp2[ib] * Yb[ib]\n",
    "        sgd = sgd / batch_size\n",
    "        \n",
    "        w = w - learn_rate * Hinv.dot(sgd)\n",
    "        \n",
    "        if isgd > burn_in and (isgd - burn_in) % sample_intv == 1:\n",
    "            w_all.append(w.ravel().tolist())\n",
    "    \n",
    "    return np.array(w_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# learn_rate = 0.01\n",
    "# batch_size = 50\n",
    "# burn_in = 1000\n",
    "# sample_intv = 500\n",
    "# n_sample = 100\n",
    "# hessian_batch = 50\n",
    "# hessian_intv = 20\n",
    "\n",
    "# w_sgd = logistic_1d_hessian_sgd(X, Y, learn_rate, batch_size, \n",
    "#                             burn_in, sample_intv, n_sample, \n",
    "#                             hessian_batch, hessian_intv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print np.mean(w_sgd, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print np.cov(w_sgd.T) * (2 * batch_size / learn_rate / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sns.jointplot(w_sgd[:,0], w_sgd[:,1], kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(w_sgd[:,0], w_sgd[:,1], '.')\n",
    "# plt.axes().set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# averaged SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def sgd_avg_infer(X_, Y_, lr, \n",
    "                  size_batch, \n",
    "                  burn_in, n_avg, n_skip, n_sample):\n",
    "    lr = float(lr)\n",
    "    \n",
    "    n, p = X_.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    X = tf.placeholder(tf.float64, [None, 2])\n",
    "    \n",
    "    Y = tf.placeholder(tf.float64, [None, 1])\n",
    "    \n",
    "    w = tf.Variable(initial_value=tf.zeros([p, 1], dtype=tf.float64), dtype=tf.float64)\n",
    "\n",
    "    log_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.matmul(X, w), Y))\n",
    "    \n",
    "    opt = tf.train.GradientDescentOptimizer(lr).minimize(log_loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    \n",
    "    n_iter = burn_in + (n_avg + n_skip) * n_sample + 3\n",
    "    \n",
    "    \n",
    "    w_avg_samples = []\n",
    "    \n",
    "    w_all = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for i_ in xrange(n_iter):\n",
    "            ind_batch = np.random.choice(range(n), size=size_batch, replace=True)\n",
    "            \n",
    "            Xb = X_[ind_batch, :].reshape((-1, p))\n",
    "            Yb = Y_[ind_batch].reshape((-1, 1))\n",
    "            \n",
    "            sess.run([opt], feed_dict={X: Xb, Y: Yb})\n",
    "            \n",
    "            if i_ >= burn_in:\n",
    "                if (i_ - burn_in) % (n_avg + n_skip) < n_avg:\n",
    "                    wb = w.eval(session=sess).ravel().tolist()\n",
    "                    w_avg_samples.append(wb)\n",
    "                elif (i_ - burn_in) % (n_avg + n_skip) == n_avg:\n",
    "                    w_avg_samples = np.array(w_avg_samples)\n",
    "                    w_avg = np.mean(w_avg_samples, axis=0)\n",
    "                    w_all.append(w_avg.ravel().tolist())\n",
    "                    w_avg_samples = []\n",
    "    \n",
    "    return np.array(w_all)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# burn_in = 10000\n",
    "\n",
    "# n_avg = 1000\n",
    "# n_skip = 10\n",
    "# n_sample = 1000\n",
    "\n",
    "# lr = 0.1\n",
    "\n",
    "# size_batch = 500\n",
    "\n",
    "\n",
    "# w_sgd_avg = sgd_avg_infer(sm.tools.add_constant(X, prepend=False), Y, lr, size_batch, burn_in, n_avg, n_skip, n_sample)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print np.mean(w_sgd_avg, axis=0)\n",
    "\n",
    "# print np.cov(w_sgd_avg.T)\n",
    "\n",
    "# print np.cov(w_sgd_avg.T) * size_batch / n * n_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sns.jointplot(w_sgd_avg[:,0], w_sgd_avg[:,1], kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(w_sgd_avg[:,0], w_sgd_avg[:,1], '.')\n",
    "# plt.axes().set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# logistic square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def logistic_square(X_, Y_, lr, \n",
    "                    size_batch, \n",
    "                    burn_in, n_avg, n_skip, n_sample):\n",
    "    lr = float(lr)\n",
    "    \n",
    "    n, p = X_.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    X1 = tf.placeholder(tf.float64, [None, 2])\n",
    "    \n",
    "    Y1 = tf.placeholder(tf.float64, [None, 1])\n",
    "    \n",
    "    w = tf.Variable(initial_value=tf.zeros([p, 1], dtype=tf.float64), dtype=tf.float64)\n",
    "\n",
    "    log_loss1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.matmul(X1, w), Y1)) + 1\n",
    "    \n",
    "    X2 = tf.placeholder(tf.float64, [None, 2])\n",
    "    \n",
    "    Y2 = tf.placeholder(tf.float64, [None, 1])\n",
    "    \n",
    "    log_loss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(tf.matmul(X2, w), Y2)) + 1\n",
    "    \n",
    "    #opt = tf.train.GradientDescentOptimizer(lr).minimize(log_loss1 * log_loss2)\n",
    "    \n",
    "    \n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=-1.0)\n",
    "    \n",
    "    log_loss1_grads_and_vars = opt.compute_gradients(log_loss1, [w])\n",
    "    \n",
    "    grad_log_loss1 = log_loss1_grads_and_vars[0][0]\n",
    "    \n",
    "    \n",
    "    w_next = tf.mul(tf.constant(-lr, dtype=tf.float64), grad_log_loss1) * log_loss2 + log_loss1_grads_and_vars[0][1]\n",
    "    \n",
    "    \n",
    "    opt_op = opt.apply_gradients([(w_next - log_loss1_grads_and_vars[0][1], log_loss1_grads_and_vars[0][1])])\n",
    "    \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    \n",
    "    n_iter = burn_in + (n_avg + n_skip) * n_sample + 3\n",
    "    \n",
    "    \n",
    "    w_avg_samples = []\n",
    "    \n",
    "    w_all = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for i_ in xrange(n_iter):\n",
    "            ind_batch1 = np.random.choice(n, size=size_batch, replace=True)\n",
    "            \n",
    "            Xb1 = X_[ind_batch1, :].reshape((-1, p))\n",
    "            Yb1 = Y_[ind_batch1].reshape((-1, 1))\n",
    "            \n",
    "            ind_batch2 = np.random.choice(n, size=min(size_batch, n), replace=False)\n",
    "            Xb2 = X_[ind_batch2, :].reshape((-1, p))\n",
    "            Yb2 = Y_[ind_batch2].reshape((-1, 1))\n",
    "            \n",
    "            sess.run([opt_op], feed_dict={X1: Xb1, Y1: Yb1, X2: Xb2, Y2: Yb2})\n",
    "            \n",
    "            if i_ >= burn_in:\n",
    "                if (i_ - burn_in) % (n_avg + n_skip) < n_avg:\n",
    "                    wb = w.eval(session=sess).ravel().tolist()\n",
    "                    w_avg_samples.append(wb)\n",
    "                elif (i_ - burn_in) % (n_avg + n_skip) == n_avg:\n",
    "                    w_avg_samples = np.array(w_avg_samples)\n",
    "                    w_avg = np.mean(w_avg_samples, axis=0)\n",
    "                    w_all.append(w_avg.ravel().tolist())\n",
    "                    w_avg_samples = []\n",
    "    \n",
    "        w_all = np.array(w_all)\n",
    "    \n",
    "        w_opt = np.mean(w_all, axis=0)\n",
    "        \n",
    "        w.assign(w_opt.reshape((-1, 1)))\n",
    "    \n",
    "        f_opt = float(sess.run(log_loss1, feed_dict={X1: X_.reshape((-1, p)), Y1: Y_.reshape((-1, 1))}))\n",
    "    \n",
    "    return w_all, f_opt\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "burn_in = 1000\n",
    "\n",
    "n_avg = 1000\n",
    "n_skip = 10\n",
    "n_sample = 1000\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "size_batch = 1000\n",
    "\n",
    "\n",
    "w_logsq, f_opt = logistic_square(sm.tools.add_constant(X, prepend=False), Y, lr, size_batch, burn_in, n_avg, n_skip, n_sample)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.22885192 -0.23179654]\n",
      "[[  3.69358041e-06  -3.61768744e-06]\n",
      " [ -3.61768744e-06   7.31101835e-06]]\n",
      "[[ 0.0369358  -0.03617687]\n",
      " [-0.03617687  0.07311018]]\n"
     ]
    }
   ],
   "source": [
    "print np.mean(w_logsq, axis=0)\n",
    "\n",
    "print np.cov(w_logsq.T)\n",
    "\n",
    "print np.cov(w_logsq.T) * size_batch / n * n_avg \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.68625403208\n"
     ]
    }
   ],
   "source": [
    "print f_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.JointGrid at 0x7fe409609ad0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.jointplot(w_logsq[:,0], w_logsq[:,1], kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(w_logsq[:,0], w_logsq[:,1], '.')\n",
    "plt.axes().set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
